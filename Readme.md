# Project Name: **[LearnGrad]**

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Languages](https://img.shields.io/badge/languages-Python%20%7C%20C++%20%7C%20Rust-orange.svg)


**[LearnGrad]** is an educational project aimed at understanding the inner workings of automatic differentiation (autograd) and neural networks. This project is a **work in progress**, and I'm building it as part of my learning journey. It provides minimalistic implementations in Python, C++, and Rust, hopefully making it an excellent resource for both beginners and advanced learners.

---

This project is inspired by libraries like [Micrograd](https://github.com/karpathy/micrograd), [smolgrad](https://github.com/smolorg/smolgrad), and [magnetron](https://github.com/MarioSieg/magnetron) and aims to demystify the core concepts of machine learning frameworks:
- **Automatic Differentiation**: How gradients are computed efficiently using computational graphs.
- **Backpropagation**: The mechanics of training neural networks.
- **Optimization**: Implementing basic optimizers like SGD and Adam.

The library is implemented in three languages:
- **Python**: For rapid prototyping and ease of use.
- **C++** : For performance optimization and low-level control.
- **Rust** :crab: : For memory safety and modern language features.

---